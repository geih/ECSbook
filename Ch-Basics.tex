\section{Basic concepts} 
\label{sec:Basics}
Action potentials and extracellular potentials are electric signals, and to understand them, we need to have some basic knowledge about the physics of electricity. 

The overview given here will ...


\subsection{\orange{GH: The physics}}


\subsubsection{\orange{GH: Electric charge}}
The fundamental quantity for electricity is the charge carried by the protons and electrons that build up the atoms that build up the material world. The proton has a charge $e$, while the electron has a charge $-e$, where $e = 1.602\times10^{-19}$ Coulomb (C) is the unit charge. 

In the brain, the charge carriers are not free electrons (or protons), but ions. Ions are atoms or molecules that have gained or donated one or several electrons, and therefore have become electrically charged. Important charge carriers in the brain are Na$^+$, K$^+$ (with charge $e$), Cl$^-$ (with charge $-e$), and $Ca^{2+}$ (with charge $2e$), which are floating around in the saline solutions that fill both the intracellular and extracellular space.

Starting at a fundamental level, a pair of charges, $q'$ and $q$, will act on each others with a force given by Coulomb's law:

\begin{equation}
F = k_e\frac{q q'}r^2, 
\label{Basics:eq:CoulombF}
\end{equation}
where $k_e = 8.99\times10^9$ N$\cdot$ m$^2\cdot$C$^{-2}$ is Coulomb's constant, and $r$ is the distance between the two charges. The direction of the force is along the line between the two charges, and the force will be repelling if the charges have the same valency (sign) and attractive if they have the opposite valency. 

If there are several point charges present, the contribution from each of them sum up linearly. We can then use Coulomb's law to compute the net force that will act on one charge $q$ in a position ${\bf r}$, by summing the contributions from all other charges $q_1, q_2, q_3 ... q_N$ in positions ${\bf r_1}, {\bf r_2}, {\bf r_3} ... {\bf r_N}$: 
\begin{equation}
{\bf F}({\bf r}) = \sum_{n=1}^N k_e q q_n \frac{{\bf r}-{\bf r_n}{|{\bf r}-{\bf r_n}|^3}.
\label{Basics:eq:CoulombFN}
\end{equation}
We have here used a boldface notation to indicate that the force and positions are vectors, entities that have both a magnitude and spatial direction. The position vector ${\bf r}$ can be visualized as an arrow from a reference point $r=0$ to the position of the charge $q$. Likewise, the vector ${\bf r}-{\bf r_n}$ as an arrow between the positions of the charge pair $q$ and $q_n$, defining both the distance and direction of the (imagined) line connecting them.

If our system of study consisted of a small number $N_{small}$ of charges, we could use $N_{small}$ instances of eq. \ref{Basics:eq:CoulombFN} to compute the force acting on each individual charge. Together with Newton's law ${\bf F} = m {\bf a}$, which tells us how the charges will be accelerated in the force direction, eq.\ref{Basics:eq:CoulombFN} would then allow us to compute the movements of all our charges over time. However, when trying to understand what is going on in the brain, we are usually not interested such microscopic interactions between a small number of charges, but rather the joint interactions of a very, very large number of particles. It is then not feasible to keep track of the motion of each individual charge. 

At the larger scale, it is therefore more useful to work with electric fields, which we will define below. It is still nice to have taken a look at  Eq. \ref{Basics:eq:CoulombFN}, since it establishes the fundamental origin of electrical forces and fields. 


\subsubsection{\orange{GH: Electric fields}}
The electric field can be defined as the force that will act on a reference charge $q$, i.e., 
\begin{equation}
{\bf E}({\bf r}) = {\bf F}({\bf r})/q.
\label{Basics:eq:E}
\end{equation}

In general, an electric field can originate either from electric charges, or from time-varying magnetic fields. However, we shall assume that the problems that we deal with in this book are \textit{quasi-electrostatic}, which means that the latter contribution can be neglected. Electric fields are then exclusively due to the forces given by eq. \ref{Basics:eq:CoulombFN}, and by inserting eq. \ref{Basics:eq:CoulombFN} into eq. \ref{Basics:eq:E}, we get:
\begin{equation}
{\bf E}({\bf r}) = \sum_{n=1}^N k_e q_n \frac{{\bf r}-{\bf r_n}{|{\bf r}-{\bf r_n}|^3}.
\label{Basics:eq:CoulombEN}
\end{equation}

Eq. \ref{Basics:eq:CoulombEN} is valid down to a microscopic level, and the field predicted by it will fluctuate vividly on a very fine spatial scale. It will be big at all locations that are close to a charge (i.e., where ${|{\bf r}-{\bf r_n}|$ for some $n$ is small), and smaller at points where the distance to the nearest charge is longer. As we argued in the previous subsection, it is not a feasible to keep track of each individual charge in a macroscopic system. Apart from revealing the origin of the electrical field, eq. \ref{Basics:eq:CoulombEN} is therefore not very useful for us. 

Fortunately, these microscopic field fluctuations are not of too much interest for us when trying to understand the brain. The electrodes that we use when recording brain signals have a spatial extension, and their tip diameter are typically on the order of a micrometer or so. Hence, when we record some signal in the brain, we practically record the average signal over an electrode surface of, say, a square micrometer. 


Experimentally, when we insert an electrode into the brain, the electrode has some spatial extension. 

The diameter of a typical electrode may be in the order of some micrometers, which 





The field given by 

{\bf r}-{\bf r_n}







\subsubsection{\orange{GH: Electric potentials}}
An often more practically useful concept that the electrical field, is the electrical potential $\phi$ (with units Volt (V)), which is what we normally measure experimentally when we stick an electrode into the brain. In the quasi-static regime, the electric field can be expressed as the spatial derivative of the potential:
\begin{equation}
{\bf E}(x,y,z) = - \nabla \phi(x,y,z) = - \left(\frac{d\phi}{dx} {\bf i}  + \frac{d\phi}{dy} {\bf j} + \frac{d\phi}{dz} {\bf k} \right)
\phi 
\label{Basics:eq:EV}
\end{equation}
The spatial derivative $\nabla$ computes how much a property changes in the various spatial directions, and ${\bf i}$, ${\bf j}$ and  ${\bf k}$ are the unit vectors in the three spatial directions $x$, $y$ and $z$, respectively. 

To get an intuitive understanding of the electric potential, it helps to consider an idealized one-dimensional scenario with a constant field in the $x$-direction. Then, eq. \ref{Basics:eq:EV} simplifies to,
\begin{equation}
E(x) = -\frac{d\phi(x)}{dx} = -\frac{\Delta \phi}{\Delta x} = -\frac{\phi(x_b)-\phi(x_a)}{x_b-x_a},
\label{Basics:eq:EV1D}
\end{equation}
where the first equality follows from the 1D-assumption, the second from the assumption that $E$ is constant, and the third is simply a definition of the second, where $x_b$ and $x_a$ may represent any two arbitrary points in space. For example, if $E = 1$ V/m, and if the distance between our points $x_b-x_a$ is 1m, Eq. \ref{Basics:eq:EV1D} tells us that $\phi$ will be 1 V  lower in $x_b$ compared to $x_a$.

The motivation for introducing the last example, was that we wanted use it to make a comment on \textit{grounding}. In the example, the same field $E$ can be obtained with any pair of potentials $\phi_a$ and $\phi_b$ as long as the difference between them is 1 V. We can therefore not speak of the potential as an absolute quantity, only of the potential \textit{difference} between two points. When we record potential in a given location, we therefore always record it relative to some an arbitrary reference point, which we normally call \textit{ground}, and where we define $\phi = 0$. When recording extracellular potential, the reference electrode can be placed either outside or inside brain tissue \cite{Sharott2015}, but typically sufficiently far away that one can assume that the potential at the reference electrode is not affected by the processes that one wishes to investigate with the measuring electrode. 


\subsubsection{\orange{GH: Electric current}}

Eq. \ref{Basics:eq:CoulombE}

\ref{Basics:eq:CoulombE2}

A test charge $q$ in a medium will of course be exposed to the fields from a large number of other charges. 





The force that a charge excerts on other charges can be expressed as an electric field.

A charge is surrounded by 

Fundamentally, the electric field can be defined as the force that will act on a reference charge $q$, i.e., ${\bf E} = {\bf F}/q$. Now, if we existed in a vacuum where $q$ was the only charge present, this relation implies that a constant field ${\bf E}$ would give a constant acceleration of $q$ (since ${\bf F} = m{\bf a}$) in the field-direction. 

In a macroscopic system, however, the constant acceleration will only go on for a very tiny time period (called the charge relaxation-time) before our protagonist charge $q$ will bump into some other particle and be scattered out in some random direction. After the scattering event, the acceleration will start "from scratch" again, and go on until the next collision takes place, and so on. Whereas the scattering events will make the motion of $q$ random, the small periods of acceleration between them will at average give $q$ a net drift velocity in the field direction. As the same will happen for all other charges that are present, this gives rise to a drift of charge in field direction. This (average) drift will not be in constant acceleration, but rather at constant velocity, and is what constitutes the current density given by Eq. \ref{Basics:eq:i}, which is often referred to as the drift current density. 


In addition to originating from 




\subsubsection{\orange{GH: Currents}}

What we deal with at a more macroscopic level is therefore usually the "average" movement of the charges, i.e., the electrical current ${\bf I}$, which has the SI unit Ampere (A = C/s). It is often more convenient to express currents as current densities, ${\bf i}$ (A/(m$^2$)), which is simply the current per unit cross section area. 

Throughout most of this book, we shall assume that current densities are given by the formula:
\begin{equation}
{\bf i} = \sigma {\bf E}
\label{Basics:eq:i}
\end{equation}
which is a version of Ohms law that, apart from the current density, contains two fundamental quantities that we need to establish an understanding of. The conductivity $\sigma$ (with units Siemens per square meters (S/m$^2$)) expresses how well the medium conducts a current. This is a material property, and in brain tissue, it is often assumed to be a constant, at least within a given brain region. The electric field {\bf E} (with units Newton per Coulomb (N/C) or Volt per meter (V/m)) is the driving force for the current.


\subsubsection{\orange{GH: From neurodynamics to electric fields}}
The link between neural activity and the extracellular fields, come from the fact that currents generally move in loops. 





\subsection{\orange{GH: The physics}}
The basic unit for the physics of electricity is the unit charge. 


\subsection{\orange{GH: The mathematics}}
The basic unit for mathematics is the number 1.




This book is divided into two parts. In Part 1 we introduce the theory for modeling neurons and the extracellular potential that they give rise to, and in Part 2 we apply this theory to extract key insights about how to interpret extracellular potentials in terms of what aspects of neural activity that they reflect.

Throughout most parts of this book, we shall compute extracellular potentials using a two-step procedure:  

\begin{itemize}
\item {\bf Step 1:} Compute the electrical activity of the cells believed to contribute to the extracellular potential. 
\item {\bf Step 2:} Compute the extracellular potential that arises from a given, underlying cellular activity.
\end{itemize}

Following this two-step procedure, Chapter \ref{sec:Neuron} describes how to model and compute the dynamics of morphologically complex neurons (step 1), and Chapter \ref{sec:VC} describes how to compute the resulting extracellular potential (step 2). The computations in step 2 depend on the extracellular medium, as reflected through its conductivity $\sigma$. Chapter \ref{sec:Sigma} is devoted to present experimental and theoretical estimates of $\sigma$, and to explain how various choices for $\sigma$ can be incorporated into the theory (step 2). Taken together, chapters \ref{sec:Neuron}-\ref{sec:Sigma} contain the theory used for all simulations in the application part (Part 2) of this book, which remains the standard theory used within the field of neuroscience to simulate extracellular potentials. 

The standard theory (covered by Chapters \ref{sec:Neuron}-\ref{sec:Sigma}) assumes (1) that ion concentrations in the extracellular (and intracellular) environment do not vary with time. This is thought to be a good approximation during normal cellular activity. However, extracellular ion concentration shifts are a trademark of many pathological conditions such as epilepsy, stroke or spreading depression \citep{Somjen2001, Frohlich2008, Zandt2015review, Ayata2015}. In Chapter \ref{sec:Eldiff} we expand step 2 by outlining a theory for modeling extracellular ion concentration dynamics surrounding active neurons, and the effects that this will have on the extracellular potential. 

The standard theory also assumes (2) that the extracellular potential (computed in step 2) does not have any feedback effect on the neurodynamics (computed in step 1). The justification making such an assumption is that $\phi$ is usually so much smaller than the membrane potential such, so-called ephaptic effects can be neglected without any severe loss in accuracy. Also, assuming this, simplifies the mathematical framework, and speeds up computations substantially. 

In reality, both in the the extracellular potential and in the extracellular ion concentrations will in principle affect the neurodynamics. Although these effects may be small for most scenarios, there are also scenarios where the assumptions (1)-(2) do not hold, so that the standard two-step procedure can not be applied. We therefore end the theory part (Part 1) of this book by giving a summary of alternative available frameworks for computing extracellular potentials and ion concentrations (Chapter \ref{sec:Schemes}).

In the application part (Part 2) of this book, we use the theory from Part 1 to simulate extracellular potentials at different levels through forward modeling. By \textit{forward modeling} we here mean that start off with a system of neurons that is supposed to mimic a physiologically realistic scenario, and then use this predict the resulting extracellular potential, i.e., a signal that one can measure experimentally. The opposite approach, \textit{inverse modeling}, would be to try to predict the system of neurons or some properties from it from a (measured) extracellular potential. \textit{Inverse modeling} is more problematic than forward modeling, as it has no unique solution, and we will not deal with this in this book.

The motivation behind forward models is multifold. Firstly, if the model system generates an extracellular potential similar to that measured in given experimental condition, it gives validity to the model system. Secondly, by manipulating aspects of the model system, such as neuronal morphology, synapse distributions, neural membrane mechanisms etc., and exploring the effects that this has on the extracellular potential, we may gain knowledge into how various aspects of neuronal dynamics are involved in shaping the extracellular potential. As such, forward modeling gives us insights that can be used to interpret experimentally recorded extracellular potentials. 

What the extracellular potential tells us about the underlying neurodynamics depends on where it was recorded, and what aspects of it that we look at, e.g., if we look at the fast variations of it (high-frequency component), or slower variations (low-frequency component). Part 2 is organized so that we first consider the extracellular potential close to its sources, i.e., amidst the forest of neurons, which can be recorded experimentally with sharp electrodes inserted into the brain tissue. By performing a filtering of this signal, we can split it into a high-frequency part and a low-frequency part. In Chapter \ref{sec:Spikes} we examine the high-frequency part, often called the multi-unit-activity (MUA), and show that this mostly reflects the population spiking activity. In In Chapter \ref{sec:LFP} we focus on the low-frequency part, and show that this mostly reflects synaptic input to populations of pyramidal cells.

In the following chapters, we study the extracellular potential at positions further away from their neuronal sources. In Chapter \ref{sec:ECoG}, we the electrocorticogram (ECoG), measured at the surface of the cortex, or the electroencephalogram (EEG) measured at the top of the scalp. We also include a chapter (Chapter \ref{sec:MEG}) on the magnetoencephalogram, which can be modeled on a framework similar to that used for EEG. 

The theory for how electrical potential propagate from neural sources to a recording electrode can, of course, also be applied to predict how signals will propagate from artificial sources such as stimulus electrodes. In Chapter \ref{sec:Stim} we briefly present a framework for this, as it will be useful in the context of understanding brain stimulations with e.g., deep electrodes. 

Finally, we discuss the state of the art technology for brain recordings, and comment on future outlooks and people like Elon Musk (Chapter \ref{sec:Tech}), before we end  by giving a summary of the main take-home messages from this book (Chapter \ref{sec:Last}).
